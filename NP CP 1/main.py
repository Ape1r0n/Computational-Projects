import cv2
import numpy as np
from Clustering import DBSCAN
from Edge_Detection import CannyEdgeDetector
from datetime import datetime


class ObjectTracker:
    def __init__(self, video_path: str, skip_frames: int = 5):
        print(f"\n[{datetime.now().strftime('%H:%M:%S')}] Initializing Tracker...")
        self.video_path = video_path
        self.skip_frames = skip_frames

        # Initialize detectors
        self.edge_detector = CannyEdgeDetector(
            sigma=1, kernel_size=5,
            weak_pixel=75, strong_pixel=255,
            lowthreshold=0.05, highthreshold=0.15
        )
        self.dbscan = DBSCAN(eps=7, min_pts=50)

        # Setup video properties
        cap = cv2.VideoCapture(video_path)
        self.frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        self.frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        cap.release()

        # Initialize tracking variables
        self.scale_factor = 0.5
        self.new_width = int(self.frame_width * self.scale_factor)
        self.new_height = int(self.frame_height * self.scale_factor)
        self.previous_frame = None
        self.previous_centroids = None
        self.frame_count = 0
        self.object_speeds = {}

        # A list of BGR colors for visualization
        self.colors = [
            (255, 0, 0),  # Blue
            (0, 255, 0),  # Green
            (0, 0, 255),  # Red
            (255, 255, 0),  # Cyan
            (255, 0, 255),  # Magenta
            (0, 255, 255),  # Yellow
            (128, 0, 0),  # Dark Blue
            (0, 128, 0),  # Dark Green
            (0, 0, 128),  # Dark Red
            (128, 128, 0)  # Dark Cyan
        ]

        print("Initialization complete!\n")

    def preprocess_frame(self, frame):
        frame_small = cv2.resize(frame, (self.new_width, self.new_height))
        frame_gray = cv2.cvtColor(frame_small, cv2.COLOR_BGR2GRAY) if len(frame_small.shape) == 3 else frame_small
        edges = self.edge_detector.edge_detect(frame_gray)
        return edges.astype(np.uint8)

    def get_frame_difference(self, current_frame):
        if self.previous_frame is None:
            diff = np.zeros_like(current_frame)
        else:
            diff = cv2.absdiff(current_frame, self.previous_frame)
        _, diff = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)
        return diff

    def helper2D_2_3D(self, diff_frame):
        y_coords, x_coords = np.nonzero(diff_frame)
        if len(y_coords) > 1000:
            indices = np.random.choice(len(y_coords), 1000, replace=False)
            y_coords = y_coords[indices]
            x_coords = x_coords[indices]
        return np.column_stack((x_coords, y_coords, diff_frame[y_coords, x_coords]))

    # Generated by Claude AI. I had no idea how to do what I asked for. Uncanny, resemblance?
    def create_visualization(self, frame, labels, points, speeds):
        vis_frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        unique_clusters = np.unique(labels[labels >= 0])

        for cluster_id in unique_clusters:
            cluster_mask = (labels == cluster_id)
            cluster_points = points[cluster_mask]
            centroid = np.mean(cluster_points[:, :2], axis=0).astype(int)
            color = self.colors[cluster_id % len(self.colors)]

            for x, y, _ in cluster_points:
                cv2.circle(vis_frame, (int(x), int(y)), 1, color, -1)

            if cluster_id in speeds and speeds[cluster_id]:
                speed_text = f"{speeds[cluster_id][-1]:.1f} p/f"
                cv2.putText(vis_frame, speed_text, tuple(centroid),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                cv2.putText(vis_frame, speed_text, tuple(centroid),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

        return vis_frame

    def track_objects(self, visualize=True):
        object_counts = []
        last_print = 0

        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing video...")
        cap = cv2.VideoCapture(self.video_path)
        if not cap.isOpened():
            raise ValueError(f"Could not open video: {self.video_path}")
        if visualize:
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            out = cv2.VideoWriter('Fancy Video.mp4',
                                  fourcc, 30.0 / self.skip_frames, (self.new_width, self.new_height))

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            if self.frame_count - last_print >= 5:
                progress = (self.frame_count / self.total_frames) * 100
                print(f"\rProgress: {progress:.1f}% | "
                      f"Frame: {self.frame_count}/{self.total_frames} | "
                      f"Time: {datetime.now().strftime('%H:%M:%S')}", end="")
                last_print = self.frame_count

            if self.frame_count % self.skip_frames != 0:
                self.frame_count += 1
                continue

            edges = self.preprocess_frame(frame)
            frame_diff = self.get_frame_difference(edges)

            if np.sum(frame_diff) > 1000:
                points = self.helper2D_2_3D(frame_diff)
                if len(points) >= self.dbscan.min_pts:
                    self.dbscan.dbscan(points)
                    labels = self.dbscan.labels_
                    centroids = self.dbscan.cluster_centers_
                    n_objects = len(set(labels[labels >= 0]))
                    if n_objects > 0:
                        object_counts.append(n_objects)
                        if self.previous_centroids is not None and len(centroids) > 0:
                            self._calculate_speeds(centroids)
                        if visualize:
                            vis_frame = self.create_visualization(edges, labels, points, self.object_speeds)
                            out.write(vis_frame)

                    self.previous_centroids = centroids

            self.previous_frame = edges
            self.frame_count += 1

        if visualize:
            out.release()
        cap.release()
        print("\nProcessing complete!")
        return object_counts, self.object_speeds

    def _calculate_speeds(self, current_centroids):
        if not self.previous_centroids:
            return

        curr_cents = np.array(current_centroids)
        prev_cents = np.array(self.previous_centroids)
        distances = np.sqrt(((curr_cents[:, np.newaxis, :2] - prev_cents[np.newaxis, :, :2]) ** 2).sum(axis=2))

        for i, dists in enumerate(distances):
            closest_idx = np.argmin(dists)
            min_dist = dists[closest_idx]
            speed = min_dist / (self.skip_frames * self.scale_factor)
            if i not in self.object_speeds:
                self.object_speeds[i] = []
            self.object_speeds[i].append(speed)

    def analyze_video(self, visualize=True):
        object_counts, speeds = self.track_objects(visualize)
        avg_count = np.mean(object_counts) if object_counts else 0
        avg_speeds = {obj_id: np.mean(speeds) for obj_id, speeds in self.object_speeds.items()}
        return avg_count, avg_speeds


if __name__ == "__main__":
    PATH = "test.mp4"
    tracker = ObjectTracker(PATH, skip_frames=25)
    avg_count, avg_speeds = tracker.analyze_video(visualize=True)

    print(f"\nAverage objects detected: {avg_count:.1f}")
    print("\nObject speeds (pixels/frame):")
    for obj_id, speed in avg_speeds.items():
        print(f"Object {obj_id}: {speed:.1f} p/f")
